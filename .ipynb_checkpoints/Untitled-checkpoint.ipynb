{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "import word2vec\n",
    "w2v = word2vec.load('fr_small.bin')\n",
    "\n",
    "\n",
    "#Cosine similarity between 2 words\n",
    "def sim(w1, w2):\n",
    "    try:\n",
    "        vec1, vec2 = np.array(w2v[w1]), np.array(w2v[w2])\n",
    "        norm1=np.linalg.norm(vec1)\n",
    "        norm2=np.linalg.norm(vec2)\n",
    "        return (vec1*vec2).sum()/(norm1*norm2)\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "#This funtion is applied to the distance to the topic in order to change the graph's coefficient.\n",
    "def filtre(s, strength):\n",
    "    return 1/(1+np.exp(-strength*(s-0.1)))\n",
    "        #To be improved\n",
    "\n",
    "#tokenizer. Specific handling of \\n\n",
    "def tokenize(s):\n",
    "    out = []\n",
    "    tokens = SpaceTokenizer().tokenize(s)\n",
    "    for w in tokens:\n",
    "        if w[:1]==\"\\n\":\n",
    "            out.append(\"\\n\")\n",
    "            out.append(w[1:])\n",
    "        else:\n",
    "            out.append(w)\n",
    "    return out\n",
    "\n",
    "#Markov Model class\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.dic = {}\n",
    "\n",
    "    #to train a model on a corpus\n",
    "    def train(self, text, factor=1):\n",
    "        tokens = tokenize(text)\n",
    "\n",
    "        #Add all the words in the model:\n",
    "        for i in range(len(tokens)):\n",
    "            try: \n",
    "                self.dic[tokens[i]]={}\n",
    "                pass\n",
    "            except:\n",
    "                self.dic[tokens[i]]={}\n",
    "\n",
    "        #Add the links between words\n",
    "        for i in range(len(tokens)-1):\n",
    "            try:\n",
    "                self.dic[tokens[i]][tokens[i+1]] = self.dic[tokens[i]][tokens[i+1]] + factor\n",
    "            except KeyError:\n",
    "                self.dic[tokens[i]][tokens[i+1]] = factor\n",
    "\n",
    "    #softmax function, to be applied to a model\n",
    "    def softmax(self):\n",
    "        for prev in self.dic.keys():\n",
    "            oldvec = np.array(list(self.dic[prev].values()))\n",
    "            sm = oldvec.sum()\n",
    "            #Applying softmax\n",
    "            newvec = [oldvec[i]/sm for i in range(len(oldvec))]\n",
    "            #replacing values in dic\n",
    "            i=0\n",
    "            for foll in self.dic[prev].keys():\n",
    "                self.dic[prev][foll]=newvec[i]\n",
    "                i+=1\n",
    "        return 0\n",
    "\n",
    "    #met tous les mots à 1.\n",
    "    def flat(self):\n",
    "        for prev in self.dic.keys():\n",
    "            for foll in self.dic[prev].keys():\n",
    "                self.dic[prev][foll]=1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    #to have it focus on a topic (1 word)\n",
    "    def focus(self, topic, strength=4):\n",
    "        out=[]\n",
    "        #loop over the integers:\n",
    "        for prev in self.dic.keys():\n",
    "            for foll in self.dic[prev].keys():\n",
    "                sim(foll, topic)\n",
    "                self.dic[prev][foll]*= filtre(sim(foll, topic), strength)\n",
    "                out.append(filtre(sim(foll, topic), strength))\n",
    "        return out\n",
    "    \n",
    "    def mix(self, model2):\n",
    "        dic1=self.dic\n",
    "        dic2=model2.dic\n",
    "        alpha1=1/len(dic1)\n",
    "        alpha2=1/len(dic1)\n",
    "        pass #TODO\n",
    "        \n",
    "\n",
    "    #next word, generated randomly according to the model\n",
    "    def predictNext(self, previous):\n",
    "        subModel = self.dic[previous]\n",
    "        counts = np.array([subModel[word] for word in subModel.keys()])\n",
    "        probas = (counts/sum(counts)).cumsum()\n",
    "        threshold = random.random()\n",
    "        i=0\n",
    "        while probas[i]<=threshold:\n",
    "            i+=1\n",
    "        return list(subModel.keys())[i]\n",
    "\n",
    "    #generate a sentence of 300+ characters. (the second loop finishes the sentence)\n",
    "    def generateSentence(self):\n",
    "        out = \"\"\n",
    "        currword = '\\n'\n",
    "        while(len(out)<300):\n",
    "            out += currword\n",
    "            out += \" \"\n",
    "            currword = self.predictNext(currword)\n",
    "        while(currword!=\"\\n\"):\n",
    "            out += currword\n",
    "            out += \" \"\n",
    "            currword = self.predictNext(currword)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import markov\n",
    "\n",
    "with open(\"nekfeu.txt\") as f:\n",
    "    text_nek = f.read()\n",
    "with open(\"georges-brassens.txt\") as f:\n",
    "    text_georges = f.read()\n",
    "\n",
    "model_georges = markov.Model()\n",
    "model_georges.train(text_georges)\n",
    "model_georges.train(text_nek)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Et t'appuie sur les anneaux de ceux des astres \n",
      " Mais j'vais t'prouver qu'on aime pleurer \n",
      " Les politiciens courent \n",
      " Est-ce que t’avais prévu qu’on sort \n",
      " Peu de Dubaï \n",
      " Malgré les moments d'absence \n",
      " On taffe, on revient de clébard \n",
      " J'bousille le cash tombe j'ai souffert \n",
      " Plus le macro qu’il n’y a bien \n"
     ]
    }
   ],
   "source": [
    "print(model_georges.generateSentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
